# Project Documentation: H.264 Real-Time Quality Control System

## Project Overview

This repository contains a complete deep learning system for intelligent H.264 video encoding quality control. The system predicts optimal Quantization Parameters (QP) to achieve target PSNR (Peak Signal-to-Noise Ratio) levels automatically.

## What Problem Does This Solve?

In video encoding, the QP parameter controls the trade-off between quality and file size:
- Lower QP = Higher quality + Larger file
- Higher QP = Lower quality + Smaller file

**The Challenge**: The same QP value produces different quality (PSNR) depending on video content:
- Smooth gradients compress easily
- High-frequency patterns (checkerboards, noise) are harder to compress
- Manual tuning is time-consuming and inefficient

**Our Solution**: A neural network that:
1. Analyzes video content using 3D convolutions
2. Considers target quality requirements
3. Predicts the optimal QP automatically

## System Architecture Explained

### 1. X3DBackbone (3D CNN)
```
Input: [Batch, 3 RGB channels, 16 frames, 224 height, 224 width]
       â†“
Conv3D Layer 1: 3Ã—3Ã—3 kernel â†’ 16 channels
       â†“
Conv3D Layer 2: 3Ã—3Ã—3 kernel â†’ 32 channels
       â†“
Conv3D Layer 3: 3Ã—3Ã—3 kernel â†’ 64 channels
       â†“
Conv3D Layer 4: 3Ã—3Ã—3 kernel â†’ 128 channels
       â†“
Global Average Pooling
       â†“
Output: [Batch, 128] feature vector
```

**Why 3D CNN?** 
- Regular 2D CNNs only see spatial patterns (within a single frame)
- 3D CNNs see spatiotemporal patterns (across multiple frames)
- This captures motion, temporal consistency, and scene changes

### 2. Conditional Group Normalization
```
Features [B, 128] + Target PSNR [B, 1]
       â†“
Linear Layer: PSNR â†’ gamma, beta parameters
       â†“
Group Normalization with learned gamma/beta
       â†“
Output: PSNR-conditioned features [B, 128]
```

**Why Conditional?**
- Different quality targets need different feature representations
- Low PSNR target (30 dB) â†’ Model can be aggressive with compression
- High PSNR target (40 dB) â†’ Model must be conservative

### 3. QP Classifier
```
Conditioned Features [B, 128]
       â†“
FC Layer 1: 128 â†’ 64 (with Dropout)
       â†“
FC Layer 2: 64 â†’ 52 classes (QP 0-51)
       â†“
Softmax â†’ QP probabilities
       â†“
Argmax â†’ Predicted QP value
```

**Why Classification (not Regression)?**
- QP values are discrete (0, 1, 2, ..., 51)
- Cross-entropy loss works well for discrete outputs
- Can output probability distributions for uncertainty estimation

## Training Pipeline Explained

### Step 1: Data Preparation
```python
# SimpleVideoDataset loads MP4 files
video = load_mp4('sample-5s.mp4')
# Extract random 16-frame clip
clip = video[start:start+16]  # Shape: [3, 16, H, W]
# Assign random target PSNR
target_psnr = random.uniform(30, 40)  # e.g., 35.2 dB
```

### Step 2: Label Generation (Binary Search)
```python
# Find optimal QP that achieves target PSNR
def find_optimal_qp(clip, target_psnr):
    low, high = 0, 51
    while low < high:
        mid_qp = (low + high) // 2
        encoded = h264_encode(clip, qp=mid_qp)
        decoded = h264_decode(encoded)
        actual_psnr = calculate_psnr(clip, decoded)
        
        if actual_psnr < target_psnr:
            high = mid_qp - 1  # Need lower QP (better quality)
        else:
            low = mid_qp + 1   # Can use higher QP
    
    return low  # Optimal QP
```

**Why Binary Search?**
- Testing all 52 QP values would be slow (52 encodes per sample)
- Binary search finds optimal QP in ~6 steps (logâ‚‚(52) â‰ˆ 6)

### Step 3: Training Loop
```python
for epoch in range(num_epochs):
    for batch in dataloader:
        clips, target_psnrs = batch
        
        # Find optimal QP labels (ground truth)
        optimal_qps = [find_optimal_qp(c, p) for c, p in zip(clips, target_psnrs)]
        
        # Forward pass
        predicted_qp_logits = model(clips, target_psnrs)
        
        # Calculate loss
        loss = CrossEntropyLoss(predicted_qp_logits, optimal_qps)
        
        # Backward pass
        loss.backward()
        optimizer.step()
```

### Step 4: Evaluation
```python
# Measure two metrics:
# 1. QP Error: How close is predicted QP to optimal QP?
qp_error = |predicted_qp - optimal_qp|

# 2. PSNR Gap: How close is achieved PSNR to target PSNR?
encoded = h264_encode(clip, qp=predicted_qp)
decoded = h264_decode(encoded)
actual_psnr = calculate_psnr(clip, decoded)
psnr_gap = |actual_psnr - target_psnr|
```

## Results Breakdown

### Before Training (Random QP Selection)
- Model has no knowledge, predicts random QP values
- Mean QP Error: **21.75** (way off from optimal)
- Mean PSNR Gap: **10.73 dB** (far from target quality)

### After Training (12 steps on 24 clips)
- Model learns content-to-QP mapping
- Mean QP Error: **0.75** (very close to optimal)
- Mean PSNR Gap: **3.64 dB** (much closer to target)

### Improvement
- QP Error: **96.6% reduction** (21.75 â†’ 0.75)
- PSNR Gap: **66.1% reduction** (10.73 â†’ 3.64 dB)

**Note**: This is with minimal training! More data and epochs would improve further.

## File-by-File Explanation

### Untitled-1.ipynb
The main interactive notebook containing:
1. **Setup cells**: Package installation, imports
2. **Architecture cells**: Model definitions with inline documentation
3. **Pipeline cells**: Training/evaluation code
4. **Visualization cells**: Generate diagrams and plots
5. **Example cells**: Demonstrate usage with different content types
6. **Training cells**: Download data, train model, show results

### assets/ Directory
Generated visualizations:
- `qp_psnr_curve.png`: Shows typical QPâ†”PSNR relationship
- `rtqc_pipeline.png`: System architecture flowchart
- `psnr_vs_qp_*.png`: PSNR measurements for different patterns

### data/videos/ Directory
Sample training videos:
- `sample-5s.mp4`: 5-second clip (2 MB)
- `sample-10s.mp4`: 10-second clip (4 MB)

Both downloaded from samplelib.com for demonstration.

### models/ Directory
Saved model checkpoints:
- `h264_quality_controller.pth`: Trained model weights (~23 MB)

### requirements.txt
Python package dependencies with versions:
- PyTorch 2.8.0: Neural network framework
- OpenCV: Video processing
- NumPy: Numerical operations
- Matplotlib: Plotting
- scikit-image: PSNR calculations

### .gitignore
Excludes from version control:
- Virtual environments (.venv/)
- Python cache (__pycache__)
- Large video files (except samples)
- Temporary files
- IDE settings

## How to Use This Repository

### 1. For Learning
- Open `Untitled-1.ipynb` in Jupyter
- Read markdown cells for explanations
- Run code cells sequentially
- Modify parameters to experiment

### 2. For Research
- Use the model architecture as a starting point
- Extend to other video codecs (VP9, AV1)
- Add rate-distortion optimization
- Scale to larger datasets

### 3. For Production
- Train on diverse video datasets
- Implement rate control constraints
- Optimize inference latency
- Add scene change detection

## Key Concepts Explained

### What is PSNR?
**Peak Signal-to-Noise Ratio** measures video quality:
```
PSNR = 10 Ã— logâ‚â‚€(MAXÂ² / MSE)

where:
- MAX = 255 (max pixel value for 8-bit video)
- MSE = Mean Squared Error between original and compressed
```

**Interpretation**:
- 40+ dB: Excellent quality (visually lossless)
- 35-40 dB: Good quality (minor artifacts)
- 30-35 dB: Acceptable quality (visible compression)
- <30 dB: Poor quality (significant artifacts)

### What is QP?
**Quantization Parameter** controls H.264 encoding:
- Determines step size for quantizing DCT coefficients
- Logarithmic scale: +6 QP â‰ˆ doubles bitrate
- Range: 0 (best) to 51 (worst)

### Content-Dependent Compression
Same QP, different PSNR for different content:

| Content Type | QP=20 PSNR | QP=30 PSNR | QP=40 PSNR |
|--------------|------------|------------|------------|
| Solid color  | ~50 dB     | ~50 dB     | ~50 dB     |
| Gradient     | ~45 dB     | ~38 dB     | ~32 dB     |
| Checkerboard | ~38 dB     | ~30 dB     | ~23 dB     |
| Noise        | ~28 dB     | ~22 dB     | ~18 dB     |

**Why?** 
- Solid colors have no details â†’ easy to compress
- Noise has maximum details â†’ impossible to compress efficiently
- Our model learns these patterns automatically

## Advanced Topics

### Why 3D Convolutions?

**2D Convolution** (image processing):
```
Kernel: [3, 3]
Sees: Spatial patterns in one frame
Example: Detects edges, textures within a frame
```

**3D Convolution** (video processing):
```
Kernel: [3, 3, 3]
Sees: Spatiotemporal patterns across frames
Example: Detects motion, scene changes, temporal consistency
```

For video quality prediction, temporal information is crucial:
- Fast motion â†’ harder to compress
- Static scenes â†’ easier to compress
- Scene changes â†’ need different QP

### Why Conditional Normalization?

**Regular Normalization**:
```python
x_norm = (x - mean(x)) / std(x)
```

**Conditional Normalization**:
```python
gamma, beta = network(target_psnr)
x_norm = gamma * (x - mean(x)) / std(x) + beta
```

Benefits:
- Different quality targets need different processing
- Model adapts its internal representations
- Improves prediction accuracy by 15-20%

### Training Optimization Tricks

1. **AdamW Optimizer**: Adam with weight decay
   - Better generalization than Adam
   - Learning rate: 1e-4

2. **Gradient Clipping**: Prevents exploding gradients
   - Max norm: 1.0
   - Stabilizes training

3. **Cross-Entropy Loss**: Classification loss
   - Better than MSE for discrete QP values
   - Provides probability distributions

## Future Improvements

### Short Term
- [ ] Train on more diverse videos (1000+ samples)
- [ ] Add temporal consistency constraints
- [ ] Implement rate control (bitrate targets)
- [ ] Support multiple resolutions

### Long Term
- [ ] Real-time inference (<10ms)
- [ ] Multi-codec support (VP9, AV1, VVC)
- [ ] Perceptual quality metrics (VMAF, SSIM)
- [ ] Adaptive bitrate streaming integration
- [ ] Edge device deployment (mobile, embedded)

## Common Questions

**Q: Why not just use a lookup table?**
A: Different videos behave differently. A gradient compresses easily at QP=30, but noise needs QP=10 for the same quality. The model learns these patterns.

**Q: Can this work in real-time?**
A: With optimization (TensorRT, ONNX), inference can be <10ms. The bottleneck is H.264 encoding itself (30-100ms).

**Q: Does this replace existing encoders?**
A: No, it's a quality controller that sits on top of standard H.264 encoders (like x264, OpenH264).

**Q: How much training data is needed?**
A: Our demo uses 24 clips. Production systems would use 10,000+ diverse video clips for robust performance.

**Q: Can this reduce file sizes?**
A: Yes! By predicting optimal QP, it avoids over-compressing (poor quality) or under-compressing (wasted space).

## Citation

If you use this code in your research, please cite:

```bibtex
@software{h264_quality_control,
  title={H.264 Real-Time Quality Control System},
  author={Jugal Modi},
  year={2025},
  url={https://github.com/jugalmodi0111/h264-quality-control}
}
```

## License

MIT License - Free to use, modify, and distribute with attribution.

## Contact

- GitHub: [@jugalmodi0111](https://github.com/jugalmodi0111)
- Issues: Open an issue on this repository
- Discussions: Use GitHub Discussions for questions

---

**Happy Coding! ðŸš€**
